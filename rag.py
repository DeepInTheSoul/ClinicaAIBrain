import os
import streamlit as st
from dotenv import dotenv_values
from langchain_community.embeddings.baidu_qianfan_endpoint import QianfanEmbeddingsEndpoint
from langchain_community.llms.baidu_qianfan_endpoint import QianfanLLMEndpoint
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import UnstructuredMarkdownLoader, PyPDFLoader, UnstructuredFileLoader
import codecs
import csv
config = dotenv_values(".env")


QIANFAN_AK = "rYndCW8UyNrh7ZIxAmxG0w1X"
QIANFAN_SK = "KovKWoaJeKYeIQwLgOUxFof5KI1ggTRq"
embeddings = QianfanEmbeddingsEndpoint(model='bge-large-zh', qianfan_ak=QIANFAN_AK, qianfan_sk=QIANFAN_SK)

def rag_page():
    st.title("ğŸ“šçŸ¥è¯†åº“ç®¡ç†")
    files = st.file_uploader("ä¸Šä¼ çŸ¥è¯†æ–‡ä»¶(ç›®å‰æ”¯æŒtxtã€pdfã€mdæ–‡ä»¶ï¼Œå»ºè®®å°†wordè½¬æ¢ä¸ºpdfæ–‡ä»¶)ï¼š")
    if files is not None:
        filepath = os.path.join('file', files.name)
        if filepath.lower().endswith(".csv"):
            num = 0
            with codecs.open(filepath) as f:
                new_json = []
                for row in csv.DictReader(f, skipinitialspace=True):
                    data = {}
                    data['question'] = row['ask']
                    data['answer'] = row['answer']
                    data_str = str(data)
                    num = num + 1
                    new_json.append(data_str)
                    if num > 1000:
                        break
                st.markdown("æ­£åœ¨æŠŠæ•°æ®ä¼ è‡³å‘é‡æ¨¡å‹ç¼–ç , è¯·è€å¿ƒç­‰å¾…, æš‚æ—¶å…ˆåˆ«åˆ‡æ¢ç•Œé¢")
                vector_db = Chroma.from_texts(new_json, embedding=embeddings, persist_directory="./chroma_db")
                vector_db.persist()
                st.markdown("---------------å·²å°†æ–‡ä»¶è£…è½½è¿›çŸ¥è¯†åº“ä¸­--------------------")
        else:
                docs = file_loader(files)
                konwlwdge_vec_store(docs)
                st.markdown("---------------å·²å°†æ–‡ä»¶è£…è½½è¿›çŸ¥è¯†åº“ä¸­--------------------")

def file_loader(file):
    filepath = os.path.join('file', file.name)
    with open(filepath, 'wb') as f:
        f.write(file.getbuffer())
    if filepath.lower().endswith(".md"):
        loader = UnstructuredMarkdownLoader(filepath)
        docs = loader.load()
    elif filepath.lower().endswith(".pdf"):
        loader = PyPDFLoader(filepath)
        docs = loader.load()
    elif filepath.lower().endswith(".txt"):
        loader = UnstructuredFileLoader(filepath,encoding='utf8')
        docs = loader.load()
    else:
        loader = UnstructuredFileLoader(filepath, mode="elements")
        docs = loader.load()
    return docs

def konwlwdge_vec_store(docs):

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100
    )

    splits = text_splitter.split_documents(docs)

    vector_db = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory="./chroma_db")
    vector_db.persist()



def format_docs(docs):
    return "\n\n".join(doc for doc in docs)



def rag_chain(llm):

    vector_db = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    retriever = vector_db.as_retriever()

    # åŠ è½½ä¸€ä¸ªé¢„å…ˆå®šä¹‰çš„æç¤ºç”Ÿæˆå™¨ï¼Œç”¨äºç”Ÿæˆæ£€ç´¢é—®é¢˜ã€‚
    template = """åŸºäºä»¥ä¸‹å·²çŸ¥ä¿¡æ¯ï¼Œè¯·ä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                ä¸è¦ä¹±å›ç­”ï¼Œå¦‚æœæ— æ³•ä»å·²çŸ¥ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°å‘Šè¯‰ç”¨æˆ·ã€‚
                å·²çŸ¥å†…å®¹:   
                {context}
                é—®é¢˜:
                {question}"""

    prompt = ChatPromptTemplate.from_template(template)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
    )
    return rag_chain


def generate_initial_answers(llm, question, k=1):
    """ä½¿ç”¨LLMç”Ÿæˆkä¸ªåˆæ­¥ç­”æ¡ˆ"""
    answers = []
    for _ in range(k):
        answer = llm.invoke(question)
        answers.append(answer)
    return answers

def vectorize_texts(embedding_function, texts):
    """å°†æ–‡æœ¬å‘é‡åŒ–"""
    return [embedding_function.embed_query(text) for text in texts]

def average_vectors(vectors):
    """å¹³å‡åŒ–å‘é‡"""
    if not vectors:
        return []
    avg_vector = [sum(dim) / len(vectors) for dim in zip(*vectors)]
    return avg_vector


from typing import List, Dict
import json

def filter_relevant_strings(llm, strings: List[str], question: str) -> Dict:
    """
    ä½¿ç”¨LLMè¿‡æ»¤ä¸é—®é¢˜ç›¸å…³çš„å­—ç¬¦ä¸²ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«ç›¸å…³å­—ç¬¦ä¸²åˆ—è¡¨ã€è¿‡æ»¤æ ‡å¿—ã€è§£é‡Šå’Œç›¸å…³åˆ†æ•°çš„Listã€‚

    å‚æ•°:
        llm: å·²åˆå§‹åŒ–çš„è¯­è¨€æ¨¡å‹å®ä¾‹ã€‚
        strings (List[str]): è¦è¿‡æ»¤çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
        question (str): éœ€è¦å›ç­”çš„é—®é¢˜ã€‚

    è¿”å›:
        åŒ…å«ç›¸å…³å­—ç¬¦ä¸²åˆ—è¡¨ã€æ˜¯å¦è¿‡æ»¤ã€è§£é‡Šå’Œç›¸å…³åˆ†æ•°çš„JSONã€‚
    """
    # æ„å»ºè¿‡æ»¤æç¤º
    filter_prompt = """ç»™å®šä»¥ä¸‹å­—ç¬¦ä¸²åˆ—è¡¨ä½¿ç”¨:
{strings}

è¯·æ ¹æ®ä»¥ä¸‹é—®é¢˜è¿‡æ»¤ç›¸å…³çš„å­—ç¬¦ä¸²ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«ç›¸å…³å­—ç¬¦ä¸²åˆ—è¡¨ã€æ˜¯å¦è¿‡æ»¤ã€è§£é‡Šå’Œç›¸å…³åˆ†æ•°çš„List:
é—®é¢˜: {question}
æ ¼å¼è¦æ±‚:
[
    "filtered_strings": "ç›¸å…³å­—ç¬¦ä¸²1",
    "filtered": true/false,
    "explanation": "è§£é‡Šå†…å®¹",
    "scores": åˆ†æ•°1
,

    "filtered_strings": "ç›¸å…³å­—ç¬¦ä¸²2",
    "filtered": true/false,
    "explanation": "è§£é‡Šå†…å®¹",
    "scores": åˆ†æ•°2
]"""

    # å°†å­—ç¬¦ä¸²åˆ—è¡¨æ ¼å¼åŒ–ä¸ºä¸€ä¸ªå¤šè¡Œå­—ç¬¦ä¸²
    formatted_strings = "\n".join(strings)

    # ä½¿ç”¨æç¤ºç”Ÿæˆå™¨åˆ›å»ºæç¤º
    prompt = filter_prompt.format(strings="["+formatted_strings+"]", question=question)

    # ä½¿ç”¨LLMç”Ÿæˆè¿‡æ»¤åçš„å­—ç¬¦ä¸²
    response = llm.invoke(prompt)

    # è§£æJSONå“åº”
    # result = json.loads(response)

    return response.strip("'```json\n'")


def filter_result(llm, strings: List[str], question: str) -> Dict:
    """
    ä½¿ç”¨LLMè¿‡æ»¤ä¸é—®é¢˜ç›¸å…³çš„å­—ç¬¦ä¸²ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«ç›¸å…³å­—ç¬¦ä¸²åˆ—è¡¨ã€è¿‡æ»¤æ ‡å¿—ã€è§£é‡Šå’Œç›¸å…³åˆ†æ•°çš„Listã€‚

    å‚æ•°:
        llm: å·²åˆå§‹åŒ–çš„è¯­è¨€æ¨¡å‹å®ä¾‹ã€‚
        strings (List[str]): è¦è¿‡æ»¤çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
        question (str): éœ€è¦å›ç­”çš„é—®é¢˜ã€‚

    è¿”å›:
        åŒ…å«ç›¸å…³å­—ç¬¦ä¸²åˆ—è¡¨ã€æ˜¯å¦è¿‡æ»¤ã€è§£é‡Šå’Œç›¸å…³åˆ†æ•°çš„JSONã€‚
    """
    # æ„å»ºè¿‡æ»¤æç¤º
    filter_prompt = """ç»™å®šä»¥ä¸‹å­—ç¬¦ä¸²åˆ—è¡¨ä½¿ç”¨:
{strings}

è¯·æ ¹æ®ä»¥ä¸‹é—®é¢˜è¿‡æ»¤ç›¸å…³çš„å­—ç¬¦ä¸²ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«ç›¸å…³å­—ç¬¦ä¸²åˆ—è¡¨ã€æ˜¯å¦è¿‡æ»¤ã€è§£é‡Šå’Œç›¸å…³åˆ†æ•°çš„List:
é—®é¢˜: {question}
æ ¼å¼è¦æ±‚:
[
    "filtered_strings": "ç›¸å…³å­—ç¬¦ä¸²1",
    "filtered": true/false,
    "explanation": "è§£é‡Šå†…å®¹",
    "scores": åˆ†æ•°1
,

    "filtered_strings": "ç›¸å…³å­—ç¬¦ä¸²2",
    "filtered": true/false,
    "explanation": "è§£é‡Šå†…å®¹",
    "scores": åˆ†æ•°2
]"""

    # å°†å­—ç¬¦ä¸²åˆ—è¡¨æ ¼å¼åŒ–ä¸ºä¸€ä¸ªå¤šè¡Œå­—ç¬¦ä¸²
    formatted_strings = "\n".join(strings)

    # ä½¿ç”¨æç¤ºç”Ÿæˆå™¨åˆ›å»ºæç¤º
    prompt = filter_prompt.format(strings="["+formatted_strings+"]", question=question)

    # ä½¿ç”¨LLMç”Ÿæˆè¿‡æ»¤åçš„å­—ç¬¦ä¸²
    response = llm.invoke(prompt)

    # è§£æJSONå“åº”
    # result = json.loads(response)

    return response.strip("'```json\n'")


import random
import openai
import json

def generate_prompt_results(question, answers):
    # æ„å»ºè¯„åˆ†æç¤º
    prompt = f"""
æˆ‘å°†æä¾›ä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ç­”æ¡ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†å¯¹æ¯ä¸ªç­”æ¡ˆè¿›è¡Œè¯„åˆ†ï¼š

1. **æµç•…æ€§**ï¼šç­”æ¡ˆæ˜¯å¦è¯­æ³•æ­£ç¡®ã€è¡¨è¾¾æ¸…æ™°ã€æ˜“äºç†è§£ã€‚
2. **åˆç†æ€§**ï¼šç­”æ¡ˆæ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€æä¾›çš„ä¿¡æ¯æ˜¯å¦å‡†ç¡®å’Œæœ‰ç”¨ã€‚
3. **ç»†èŠ‚ä¸°å¯Œåº¦**ï¼šç­”æ¡ˆæ˜¯å¦æä¾›äº†å……åˆ†çš„ç»†èŠ‚å’Œè§£é‡Šã€‚
4. **ç»“æ„æ€§**ï¼šç­”æ¡ˆæ˜¯å¦æœ‰è‰¯å¥½çš„é€»è¾‘ç»“æ„ï¼Œæ¡ç†æ¸…æ™°ã€‚
5. **ä¸“ä¸šæ€§**ï¼šç­”æ¡ˆæ˜¯å¦æ˜¾ç¤ºå‡ºå¯¹ä¸»é¢˜çš„æ·±åˆ»ç†è§£å’Œä¸“ä¸šçŸ¥è¯†ã€‚

è¯·ä¸ºæ¯ä¸ªç­”æ¡ˆåˆ†åˆ«ç»™å‡ºæ¯é¡¹æ ‡å‡†çš„è¯„åˆ†ï¼ˆæ»¡åˆ†10åˆ†ï¼‰ï¼Œå¹¶æä¾›ç®€çŸ­çš„è§£é‡Šã€‚æœ€ç»ˆè¾“å‡ºæ ¼å¼åº”å¦‚ä¸‹ï¼š

é—®é¢˜:
{question}
"""
    for i, answer in enumerate(answers, start=1):
        prompt += f"""
ç­”æ¡ˆ{i}:
{answer}
æµç•…æ€§è¯„åˆ†: {{fluency_score{i}}}/10
åˆç†æ€§è¯„åˆ†: {{relevance_score{i}}}/10
ç»†èŠ‚ä¸°å¯Œåº¦è¯„åˆ†: {{detail_score{i}}}/10
ç»“æ„æ€§è¯„åˆ†: {{structure_score{i}}}/10
ä¸“ä¸šæ€§è¯„åˆ†: {{expertise_score{i}}}/10
è§£é‡Š: {{explanation{i}}}
"""

    prompt += f"""
è¯·æ ¹æ®ä»¥ä¸Šæ ‡å‡†å¯¹ä»¥ä¸‹ç­”æ¡ˆè¿›è¡Œè¯„åˆ†ï¼š

é—®é¢˜: {question}
ç­”æ¡ˆ:
"""
    for i, answer in enumerate(answers, start=1):
        prompt += f"ç­”æ¡ˆ{i}: {answer}\n"

    return prompt


import re

def result_filter(answer_text):

    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…åˆ†æ•°
    pattern = r"ç­”æ¡ˆ(\d+):\s*æµç•…æ€§è¯„åˆ†: (\d+)/10\s*åˆç†æ€§è¯„åˆ†: (\d+)/10\s*ç»†èŠ‚ä¸°å¯Œåº¦è¯„åˆ†: (\d+)/10\s*ç»“æ„æ€§è¯„åˆ†: (\d+)/10\s*ä¸“ä¸šæ€§è¯„åˆ†: (\d+)/10"

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…
    matches = re.findall(pattern, answer_text)

    # åˆå§‹åŒ–åˆ†æ•°å­—å…¸
    scores = {}

    # è®¡ç®—æ¯ä¸ªç­”æ¡ˆçš„æ€»åˆ†
    for match in matches:
        answer_id = int(match[0])
        fluency = int(match[1])
        relevance = int(match[2])
        detail = int(match[3])
        structure = int(match[4])
        expertise = int(match[5])

        total_score = fluency + relevance + detail + structure + expertise
        scores[answer_id] = total_score

    # è·å–æ‰€æœ‰åˆ†æ•°çš„åˆ—è¡¨
    score_values = list(scores.values())

    # æ‰¾åˆ°æœ€å¤§å€¼
    max_score = max(score_values)

    # æ‰¾åˆ°æ‰€æœ‰æœ€å¤§å€¼çš„ç´¢å¼•ï¼ˆç­”æ¡ˆç¼–å·ï¼‰
    max_score_indices = [i for i, score in scores.items() if score == max_score]

    # é€‰æ‹©æœ€å°çš„ç´¢å¼•
    best_answer_index = min(max_score_indices)

    print(f"æœ€ä½³ç­”æ¡ˆæ˜¯ç­”æ¡ˆ{best_answer_index}ï¼Œæ€»åˆ†ä¸º{max_score}")

    # æ‰“å°æ¯ä¸ªç­”æ¡ˆçš„æ€»åˆ†
    for answer_id, total_score in scores.items():
        print(f"ç­”æ¡ˆ{answer_id}çš„æ€»åˆ†: {total_score}")

    return best_answer_index
def rag_chain_v2(llm, question):
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“å’Œæ£€ç´¢å™¨
    vector_db = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    # retriever = vector_db.as_retriever()

    # ç”Ÿæˆkä¸ªåˆæ­¥ç­”æ¡ˆ
    initial_answers = generate_initial_answers(llm, question)

    # å°†é—®é¢˜å’Œkä¸ªç­”æ¡ˆå‘é‡åŒ–
    question_vector = embeddings.embed_query(question)
    answer_vectors = embeddings.embed_documents(initial_answers)

    # å°†é—®é¢˜å‘é‡ä¸ç­”æ¡ˆå‘é‡ç›¸åŠ å¹¶å¹³å‡åŒ–
    combined_vectors = [question_vector] + answer_vectors
    avg_vector = average_vectors(combined_vectors)

    # ä½¿ç”¨å¹³å‡åŒ–åçš„å‘é‡è¿›è¡Œæ£€ç´¢ æ£€ç´¢å‡º2ä¸ªç­”æ¡ˆ
    context_docs = vector_db.similarity_search_by_vector(embedding=avg_vector,k=5)
    #ç²¾æ’ å¤§æ¨¡å‹ç­›é€‰
    document_list = json.loads(filter_relevant_strings(llm, [i.page_content for i  in context_docs ],question))

    # è¿‡æ»¤å¹¶æå– "filtered": true å¯¹åº”çš„ "filtered_strings"
    filtered_strings = [item["filtered_strings"] for item in document_list if item["filtered"]]

    formatted_context = format_docs(filtered_strings)
    result = []
    for _ in range(2):
    # æ„å»ºæç¤ºç”Ÿæˆå™¨
        template_fina = """åŸºäºä»¥ä¸‹å·²çŸ¥ä¿¡æ¯ï¼Œè¯·ä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        ä¸è¦ä¹±å›ç­”ï¼Œå¦‚æœæ— æ³•ä»å·²çŸ¥ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°å‘Šè¯‰ç”¨æˆ·ã€‚
        å·²çŸ¥å†…å®¹:
        {context}
        é—®é¢˜:
        {question}"""

        # ä½¿ç”¨æç¤ºç”Ÿæˆå™¨åˆ›å»ºæç¤º
        prompt = template_fina.format(context=formatted_context, question=question)

        # ä½¿ç”¨LLMç”Ÿæˆè¿‡æ»¤åçš„å­—ç¬¦ä¸²
        response = llm.invoke(prompt)
        result.append(response)
    # è§£æJSONå“åº”


    response = llm.invoke(generate_prompt_results(question=question,answers=result))
    return result[result_filter(response)-1]
    # selected_text = random.choice(result)
    # return selected_text


# æ„å»ºRAGé“¾

#å…ˆåˆ©ç”¨llmç”Ÿæˆ kä¸ªç­”æ¡ˆ
#åˆ©ç”¨bge å°†kä¸ªç­”æ¡ˆ å’Œ 1 ä¸ªé—®é¢˜è¿›è¡Œå‘é‡åŒ–
#å°†k+1 ä¸ª ç­”æ¡ˆå’Œé—®é¢˜å‘é‡ç›¸åŠ 
#ç„¶åå°†è¿™k+1ä¸ªå‘é‡çš„å’Œè¿›è¡Œå¹³å‡åŒ–
#å°†æ‰€å¾—å‘é‡è¿›è¡Œ å‘é‡å¬å›

#ç²¾æ’é˜¶æ®µä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œè¿›è¡Œ è¿‡æ»¤ç•™ä¸‹nge


#æœ€åç»“æœè¿›è¡Œç­›é€‰ å…ˆç”¨ä¸€ä¸ªå¤§æ¨¡å‹ç”Ÿæˆå¾ˆå¤šä¸ªç­”æ¡ˆ ç„¶åå†åˆ©ç”¨å¤§æ¨¡å‹å¯¹ç»“æœè¿›è¡Œç­›é€‰

#ç¬¬ä¸€ç§æ–¹æ³• å…ˆç”¨langchainçš„ragæ¥å£è¿›è¡Œæ”¹æ€§
#ç„¶åå¯¹langchainçš„ragæ¥å£è¿›è¡Œç²¾æ’
